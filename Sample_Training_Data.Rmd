---
title: "Sample Training and Test Data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Selecting Training and Testing Data
Randomly 20% sample the full data (without replacement) to create a testing dataset which will be set aside. The remaining 80% will constitute the training data.
```{r}
set.seed(123456)
data <- read.csv("Data/detailed.cleveland.data.csv")
test_train_split <- sample(c(FALSE,TRUE), nrow(data), replace=TRUE, prob = c(0.2, 0.8))
test_data <- data[!test_train_split,]
train_data <- data[test_train_split,]
write.csv(test_data, "Data/Cleveland.data.test.csv", row.names=FALSE)
write.csv(train_data, "Data/Cleveland.data.train.csv", row.names=FALSE)
```

Trestbps Chol Thalach Oldpeak are numerical value and others are labels. Only col 12 and 13 have missing
values (maybe omit them)
```{r}
summary(data)
head(data)
data[data == "?"] = NA
which (is.na(data),arr.ind=TRUE,)

```
 
need normalization processing. 
For Bayes
According to the characteristics of the data set itself, the skewness of the distribution of the five variables of age, thalach, trtbps, chol, and oldpeak is relatively large.
If normalization processing is not performed, it will It has a certain impact on the model fitting effect.

For KNN
The value range of each variable in this data set is quite different, for example, the value of age is from 29 to 77, and the value of chol is from 126 to 564. 
The dimensions of the two are obviously different, which requires Normalization processing makes the data more in line with the requirements of model fitting(knn).

For logi regresstion
Introduce dummy variables. if no dummy variables are introduced, the data will be It is believed that there is a size relationship between the three, 
which is not conducive to model fitting(Logi regresstion.
(We can draw some graph to have a celar view those problem)